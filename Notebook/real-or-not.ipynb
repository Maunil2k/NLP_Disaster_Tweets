{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Importing Prerequisites**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, precision_recall_curve\n\nimport re\nimport string\nimport nltk\n#nltk.download('stopwords')\n#nltk.download('wordnet')\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nlemma = WordNetLemmatizer()\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization\nfrom keras.layers.embeddings import Embedding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Loading Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Cleaning/Preprocessing**"},{"metadata":{},"cell_type":"markdown","source":"#### *Removing HTML tags, emojis and punctuation marks*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions\n\ntrain['clean_text'] = train['text'].apply(lambda x: remove_URL(x))\ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_emoji(x))\ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_html(x))\ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizing the cleaned texts.\n\ntrain['tokenized'] = train['clean_text'].apply(word_tokenize)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['lower'] = train['tokenized'].apply(\n    lambda x: [word.lower() for word in x])\n\ntrain['no_stopwords'] = train['lower'].apply(\n    lambda x: [word for word in x if word not in set(stopwords.words('english'))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['no_stopwords'] = [' '.join(map(str, l)) for l in train['no_stopwords']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nstopwords = nltk.corpus.stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load pretrained GloVe embeddings\n\nembeddings_index = dict()\nf = open('../input/glove6b50dtxt/glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len_tweet = train.no_stopwords.apply(lambda x: len(x.split())).max()\n\ntok = Tokenizer()\ntok.fit_on_texts(train.no_stopwords)\nvocab_size = len(tok.word_index) + 1\nencoded_tweet = tok.texts_to_sequences(train.no_stopwords)\npadded_tweet = pad_sequences(encoded_tweet, maxlen=max_len_tweet, padding='post')\n\nvocab_size = len(tok.word_index) + 1\n\ntweet_embedding_matrix = np.zeros((vocab_size, 50))\nfor word, i in tok.word_index.items():\n    t_embedding_vector = embeddings_index.get(word)\n    if t_embedding_vector is not None:\n        tweet_embedding_matrix[i] = t_embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Creating our model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, 50, input_length=max_len_tweet, weights=[tweet_embedding_matrix], trainable=True))\nmodel.add(LSTM(256,return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(LSTM(256))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Compiling our model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'mae'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Training our model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(padded_tweet, train.target, epochs=10, batch_size= 32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Cleaning out test model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['clean_text'] = test['text'].apply(lambda x: remove_URL(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_emoji(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_html(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_punct(x))\n\ntest['tokenized'] = test['clean_text'].apply(word_tokenize)\n\ntest['lower'] = test['tokenized'].apply(\n    lambda x: [word.lower() for word in x])\n\ntest['no_stopwords'] = test['lower'].apply(\n    lambda x: [word for word in x if word not in set(nltk.corpus.stopwords.words('english'))])\n\ntest['no_stopwords'] = [' '.join(map(str, l)) for l in test['no_stopwords']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len_test = test.no_stopwords.apply(lambda x: len(x.split())).max()\n\ntok_test = Tokenizer()\ntok_test.fit_on_texts(test.no_stopwords)\nvocab_size_test = len(tok_test.word_index) + 1\nencoded_test = tok_test.texts_to_sequences(test.no_stopwords)\npadded_test = pad_sequences(encoded_test, maxlen=max_len_test, padding='post')\n\nvocab_size_test = len(tok_test.word_index) + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Making Predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(padded_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=[1 if i>0.5 else 0 for i in preds]\n#preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.DataFrame()\nsubmission['id']=test['id'].to_list()\nsubmission['target']=pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Generate a CSV file of our predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}